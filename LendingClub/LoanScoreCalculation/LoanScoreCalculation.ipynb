{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate score for customers payment history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ed2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.unacceptable_rated_pts\",0)\n",
    "spark.conf.set(\"spark.sql.very_bad_rated_pts\",100)\n",
    "spark.conf.set(\"spark.sql.bad_rated_pts\",250)\n",
    "spark.conf.set(\"spark.sql.good_rated_pts\",500)\n",
    "spark.conf.set(\"spark.sql.very_good_rated_pts\",650)\n",
    "spark.conf.set(\"spark.sql.excellent_rated_pts\",800)\n",
    "spark.conf.set(\"spark.sql.unacceptable_grade_pts\",750)\n",
    "spark.conf.set(\"spark.sql.very_bad_grade_pts\", 1000)\n",
    "spark.conf.set(\"spark.sql.bad_grade_pts\",1500)\n",
    "spark.conf.set(\"spark.sql.good_grade_pts\",2000)\n",
    "spark.conf.set(\"spark.sql.very_good_grade_pts\",2500)\n",
    "\n",
    "unacceptable_grade=\"F\"\n",
    "very_bad_grade=\"E\"\n",
    "bad_grade=\"D\"\n",
    "good_grade=\"C\"\n",
    "very_good_grade=\"B\"\n",
    "excellent_grade=\"A\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f806f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"/Users/swecha34@outlook.com/Lending_Loans/Adhoc_Topics/Env_config\"\n",
    "\n",
    "account_df=spark.read.parquet(f\"{cleaned_files}account_details\")\n",
    "loan_df=spark.read.parquet(f\"{cleaned_files}loan_details\")\n",
    "payment_df=spark.read.parquet(f\"{cleaned_files}payment_details\")\n",
    "loan_def_df=spark.read.parquet(f\"{cleaned_files}loan_defaulters\")\n",
    "customer_df=spark.read.parquet(f\"{cleaned_files}customer_details\")\n",
    "\n",
    "customer_df.createOrReplaceTempView(\"customer_details\")\n",
    "\n",
    "payment_df.createOrReplaceTempView(\"payment_details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0810e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "payment_last_df = spark.sql(\"select c.member_id, c.state, c.country, c.first_name, c.last_name, \\\n",
    "  case \\\n",
    "    when p.last_payment_amount < (p.installment * 0.5) then ${spark.sql.very_bad_rated_pts} \\\n",
    "    when p.last_payment_amount >= (p.installment * 0.5) and p.last_payment_amount < p.installment then ${spark.sql.bad_rated_pts} \\\n",
    "    when (p.last_payment_amount = (p.installment)) then ${spark.sql.good_rated_pts} \\\n",
    "    when p.last_payment_amount > (p.installment) and p.last_payment_amount <= (p.installment * 1.50) then ${spark.sql.very_good_rated_pts} \\\n",
    "    when p.last_payment_amount > (p.installment * 1.50) then ${spark.sql.excellent_rated_pts} \\\n",
    "    else ${spark.sql.unacceptable_rated_pts} \\\n",
    "  end as last_payment_pts, \\\n",
    "  case \\\n",
    "    when p.total_payment_recorded >= (p.funded_amount_investor * 0.50) then ${spark.sql.very_good_rated_pts} \\\n",
    "    when p.total_payment_recorded < (p.funded_amount_investor * 0.50) and p.total_payment_recorded > 0 then ${spark.sql.good_rated_pts} \\\n",
    "    when p.total_payment_recorded = 0 or (p.total_payment_recorded) is null then ${spark.sql.unacceptable_rated_pts} \\\n",
    "    end as total_payment_pts \\\n",
    "from payment_details p \\\n",
    "inner join customer_details c on c.member_id = p.member_id\")\n",
    "\n",
    "payment_last_df.createOrReplaceTempView(\"payment_points_df\")\n",
    "\n",
    "spark.sql(\"select * from payment_points_df where last_payment_pts!= 500 or total_payment_pts!=500 \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loan_def_df.createOrReplaceTempView(\"loan_default_details\")\n",
    "\n",
    "loan_default_pts = spark.sql(\n",
    "    \"SELECT p.*, \\\n",
    "    CASE \\\n",
    "    WHEN l.defaulters_2yrs = 0 THEN ${spark.sql.excellent_rated_pts} \\\n",
    "    WHEN l.defaulters_2yrs BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\n",
    "    WHEN l.defaulters_2yrs BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    WHEN l.defaulters_2yrs > 5 OR l.defaulters_2yrs IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "    END AS delinq_pts, \\\n",
    "    CASE \\\n",
    "    WHEN l.public_records = 0 THEN ${spark.sql.excellent_rated_pts} \\\n",
    "    WHEN l.public_records BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\n",
    "    WHEN l.public_records BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    WHEN l.public_records > 5 OR l.public_records IS NULL THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    END AS public_records_pts, \\\n",
    "    CASE \\\n",
    "    WHEN l.public_records_bankruptcies = 0 THEN ${spark.sql.excellent_rated_pts}  \\\n",
    "    WHEN l.public_records_bankruptcies BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\n",
    "    WHEN l.public_records_bankruptcies BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    WHEN l.public_records_bankruptcies > 5 OR l.public_records_bankruptcies IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "    END AS public_bankruptcies_pts, \\\n",
    "    CASE \\\n",
    "    WHEN l.enquiries_6mnths = 0 THEN ${spark.sql.excellent_rated_pts} \\\n",
    "    WHEN l.enquiries_6mnths BETWEEN 1 AND 2 THEN ${spark.sql.bad_rated_pts} \\\n",
    "    WHEN l.enquiries_6mnths BETWEEN 3 AND 5 THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    WHEN l.enquiries_6mnths > 5 OR l.enquiries_6mnths IS NULL THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "    END AS enq_pts, \\\n",
    "    CASE \\\n",
    "    WHEN l.hardship_flag = 'N' THEN ${spark.sql.very_good_rated_pts} \\\n",
    "    WHEN l.hardship_flag = 'Y' OR l.hardship_flag IS NULL THEN ${spark.sql.bad_rated_pts} \\\n",
    "    END AS hardship_pts \\\n",
    "    FROM loan_default_details l \\\n",
    "    LEFT JOIN payment_points_df p ON p.member_id = l.member_id\"\n",
    ")\n",
    "\n",
    "loan_default_pts.createOrReplaceTempView(\"loan_default_points_df\")\n",
    "\n",
    "loan_df.createOrReplaceTempView(\"loan_details\")\n",
    "\n",
    "account_df.createOrReplaceTempView(\"account_details\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "financial_df = spark.sql(\"SELECT ldef.*, \\\n",
    "    CASE \\\n",
    "        WHEN LOWER(l.loan_status) LIKE '%fully paid%' THEN ${spark.sql.excellent_rated_pts} \\\n",
    "        WHEN LOWER(l.loan_status) LIKE '%current%' THEN ${spark.sql.good_rated_pts} \\\n",
    "        WHEN LOWER(l.loan_status) LIKE '%in grace period%' THEN ${spark.sql.bad_rated_pts} \\\n",
    "        WHEN LOWER(l.loan_status) LIKE '%late (16-30 days)%' OR LOWER(l.loan_status) LIKE '%late (31-120 days)%' THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "        WHEN LOWER(l.loan_status) LIKE '%charged off%' THEN ${spark.sql.unacceptable_rated_pts} \\\n",
    "    END AS loan_status_pts, \\\n",
    "    CASE \\\n",
    "        WHEN LOWER(a.home_ownership) LIKE '%own%' THEN ${spark.sql.excellent_rated_pts} \\\n",
    "        WHEN LOWER(a.home_ownership) LIKE '%rent%' THEN ${spark.sql.good_rated_pts} \\\n",
    "        WHEN LOWER(a.home_ownership) LIKE '%mortgage%' THEN ${spark.sql.bad_rated_pts} \\\n",
    "        WHEN LOWER(a.home_ownership) LIKE '%any%' OR LOWER(a.home_ownership) IS NULL THEN ${spark.sql.very_bad_rated_pts} \\\n",
    "    END AS home_pts,  \\\n",
    "    CASE \\\n",
    "        WHEN l.funded_amount <= (a.total_high_credit_limit * 0.10) THEN ${spark.sql.excellent_rated_pts}  \\\n",
    "        WHEN l.funded_amount > (a.total_high_credit_limit * 0.10) AND l.funded_amount <= (a.total_high_credit_limit * 0.20)  THEN ${spark.sql.very_good_rated_pts}  \\\n",
    "        WHEN l.funded_amount > (a.total_high_credit_limit * 0.20) AND l.funded_amount <= (a.total_high_credit_limit * 0.30)  THEN ${spark.sql.good_rated_pts}  \\\n",
    "        WHEN l.funded_amount > (a.total_high_credit_limit * 0.30) AND l.funded_amount <= (a.total_high_credit_limit * 0.50)  THEN ${spark.sql.bad_rated_pts}  \\\n",
    "        WHEN l.funded_amount > (a.total_high_credit_limit * 0.50) AND l.funded_amount <= (a.total_high_credit_limit * 0.70)  THEN ${spark.sql.very_bad_rated_pts}  \\\n",
    "        WHEN l.funded_amount > (a.total_high_credit_limit * 0.70) THEN ${spark.sql.unacceptable_rated_pts}  \\\n",
    "    END AS credit_limit_pts, \\\n",
    "    CASE \\\n",
    "        WHEN (a.grade) ='A' and (a.sub_grade)='A1' THEN ${spark.sql.excellent_rated_pts}  \\\n",
    "        WHEN (a.grade) ='A' and (a.sub_grade)='A2' THEN (${spark.sql.excellent_rated_pts}* 0.80)  \\\n",
    "        WHEN (a.grade) ='A' and (a.sub_grade)='A3' THEN (${spark.sql.excellent_rated_pts}* 0.60)  \\\n",
    "        WHEN (a.grade) ='A' and (a.sub_grade)='A4' THEN (${spark.sql.excellent_rated_pts}* 0.40)  \\\n",
    "        WHEN (a.grade) ='A' and (a.sub_grade)='A5' THEN (${spark.sql.excellent_rated_pts}* 0.20)  \\\n",
    "        WHEN (a.grade) ='B' and (a.sub_grade)='B1' THEN (${spark.sql.very_good_rated_pts})  \\\n",
    "        WHEN (a.grade) ='B' and (a.sub_grade)='B2' THEN (${spark.sql.very_good_rated_pts}* 0.80)  \\\n",
    "        WHEN (a.grade) ='B' and (a.sub_grade)='B3' THEN (${spark.sql.very_good_rated_pts}* 0.60)  \\\n",
    "        WHEN (a.grade) ='B' and (a.sub_grade)='B4' THEN (${spark.sql.very_good_rated_pts}* 0.40)  \\\n",
    "        WHEN (a.grade) ='B' and (a.sub_grade)='B5' THEN (${spark.sql.very_good_rated_pts}* 0.20)  \\\n",
    "        WHEN (a.grade) ='C' and (a.sub_grade)='C1' THEN (${spark.sql.good_rated_pts})  \\\n",
    "        WHEN (a.grade) ='C' and (a.sub_grade)='C2' THEN (${spark.sql.good_rated_pts}* 0.80)  \\\n",
    "        WHEN (a.grade) ='C' and (a.sub_grade)='C3' THEN (${spark.sql.good_rated_pts}* 0.60)  \\\n",
    "        WHEN (a.grade) ='C' and (a.sub_grade)='C4' THEN (${spark.sql.good_rated_pts}* 0.40)  \\\n",
    "        WHEN (a.grade) ='C' and (a.sub_grade)='C5' THEN (${spark.sql.good_rated_pts}* 0.20)  \\\n",
    "        WHEN (a.grade) ='D' and (a.sub_grade)='D1' THEN (${spark.sql.bad_rated_pts})  \\\n",
    "        WHEN (a.grade) ='D' and (a.sub_grade)='D2' THEN (${spark.sql.bad_rated_pts}*0.80)  \\\n",
    "        WHEN (a.grade) ='D' and (a.sub_grade)='D3' THEN (${spark.sql.bad_rated_pts}*0.60)  \\\n",
    "        WHEN (a.grade) ='D' and (a.sub_grade)='D4' THEN (${spark.sql.bad_rated_pts}*0.40)  \\\n",
    "        WHEN (a.grade) ='D' and (a.sub_grade)='D5' THEN (${spark.sql.bad_rated_pts}*0.20)  \\\n",
    "        WHEN (a.grade) ='E' and (a.sub_grade)='E1' THEN (${spark.sql.very_bad_rated_pts})  \\\n",
    "        WHEN (a.grade) ='E' and (a.sub_grade)='E2' THEN (${spark.sql.very_bad_rated_pts}*0.80)  \\\n",
    "        WHEN (a.grade) ='E' and (a.sub_grade)='E3' THEN (${spark.sql.very_bad_rated_pts}*0.60)  \\\n",
    "        WHEN (a.grade) ='E' and (a.sub_grade)='E4' THEN (${spark.sql.very_bad_rated_pts}*0.40)  \\\n",
    "        WHEN (a.grade) ='E' and (a.sub_grade)='E5' THEN (${spark.sql.very_bad_rated_pts}*0.20)  \\\n",
    "        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F1','G1') THEN (${spark.sql.unacceptable_rated_pts})  \\\n",
    "        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F2','G2') THEN (${spark.sql.unacceptable_rated_pts}*0.80)  \\\n",
    "        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F3','G3') THEN (${spark.sql.unacceptable_rated_pts}*0.60)  \\\n",
    "        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F4','G4') THEN (${spark.sql.unacceptable_rated_pts}*0.40)  \\\n",
    "        WHEN (a.grade) in ('F','G') and (a.sub_grade) in ('F5','G5') THEN (${spark.sql.unacceptable_rated_pts}*0.20)  \\\n",
    "    END AS grade_pts \\\n",
    " FROM loan_default_points_df ldef \\\n",
    " LEFT JOIN loan_details l ON ldef.member_id = l.member_id \\\n",
    " LEFT JOIN account_details a ON a.member_id = ldef.member_id\")\n",
    "\n",
    "\n",
    "financial_df.createOrReplaceTempView(\"loan_score_details\")\n",
    "\n",
    "financial_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score = spark.sql(\"SELECT member_id, first_name, last_name, state, country, \\\n",
    "((last_payment_pts+total_payment_pts)*0.20) as payment_history_pts, \\\n",
    "((delinq_pts +public_records_pts+public_bankruptcies_pts+enq_pts+hardship_pts)*0.45) as defaulters_history_pts, \\\n",
    "((loan_status_pts+home_pts+credit_limit_pts+grade_pts)*0.35) as financial_health_pts \\\n",
    "FROM loan_score_details\")\n",
    "\n",
    "\n",
    "loan_score.createOrReplaceTempView(\"loan_score_pts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81259787",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_final=spark.sql(\"select ls.member_id,ls.first_name,ls.last_name,ls.state,ls.country, \\\n",
    "(payment_history_pts+defaulters_history_pts+financial_health_pts) as loan_score \\\n",
    "from loan_score_pts ls \")\n",
    "\n",
    "loan_score_final.createOrReplaceTempView(\"loan_score_eval\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7195fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_score_final=spark.sql(\"select ls.*, \\\n",
    "case \\\n",
    "WHEN loan_score > ${spark.sql.very_good_grade_pts} THEN '\" + excellent_grade + \"' \\\n",
    "WHEN loan_score <= ${spark.sql.very_good_grade_pts} AND loan_score > ${spark.sql.good_grade_pts} THEN '\" + very_good_grade + \"' \\\n",
    "WHEN loan_score <= ${spark.sql.good_grade_pts} AND loan_score > ${spark.sql.bad_grade_pts} THEN '\" + good_grade + \"' \\\n",
    "WHEN loan_score <= ${spark.sql.bad_grade_pts} AND loan_score > ${spark.sql.very_bad_grade_pts} THEN '\" + bad_grade + \"' \\\n",
    "WHEN loan_score <= ${spark.sql.very_bad_grade_pts} AND loan_score > ${spark.sql.unacceptable_grade_pts} THEN '\" + very_bad_grade + \"' \\\n",
    "WHEN loan_score <= ${spark.sql.unacceptable_grade_pts} THEN '\" + unacceptable_grade + \"' \\\n",
    "end as loan_final_grade \\\n",
    "from loan_score_eval ls\")\n",
    "\n",
    "\n",
    "loan_score_final.createOrReplaceTempView(\"loan_final_table\")\n",
    "\n",
    "\n",
    "spark.sql(\"select * from loan_final_table where loan_final_grade in ('B') \").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c48b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL TABLE lending_loan_e2e.customers_loan_score\n",
    "USING PARQUET\n",
    "LOCATION '/mnt/datasetbigdata/processed-data/lending-loan/customer-transformations/customers_loan_score'\n",
    "\n",
    "select * from loan_final_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
